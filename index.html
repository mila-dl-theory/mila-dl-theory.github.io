<!DOCTYPE html>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-58623028-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-58623028-3');
</script>


<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta Tags -->


<title>Mila Deep Learning Theory</title>

<meta name="description" content="Mila Deep Learning Theory Group">




<!-- Main Style -->
<link href="./files/main.css" rel="stylesheet">


</head>


<body>


<!-- Header -->
<header>
  
<br>
<div class="header_text">
      <h2>Mila Deep Learning Theory Group</h2>
       <h4>Every Thursday at 11:00 AM, EST</h4>
</div>
</header>

<br>

  <div class="page">
<!-- 
<h3  >Announcement</h3>
<p>
We plan to meet in room ICCS146 on May 25 at 5:00 PM to discuss what topics to cover this summer. 
  </p> -->

<!-- <p id="demo"></p> -->

<h3>Introduction</h3>

<p>
Mila's Deep Learning Theory Group is a discussion group aimed at keeping up with the latest research, and collaborating and brainstorming to push the boundaries of theoretical aspects of deep learning. We meet every other Thursday at 11:00 AM EST. The guidelines and schedule are provided in this website while poolings and internal discussions are conducted in <a href='https://join.slack.com/share/zt-fpcg0841-7i2sZPEdf~rzR8PgsSEwrA'> Mila's Slack Channel </a>.
</p>

<h3> Guidelines</h3>

<p>
<b>Structure:</b>
Each meeting is managed by one or a few leaders. Based on the topic, the leader will have a ~10-20 minutes presentation about the background. The leader can choose to present the topic on high-level or detailed. For example the leader can choose to
<ul>
<li>summarize ideas in a specific paper.</li>
<li>or describe high-level background and key challenges on the topic of research and the high-level approaches to tackle the challenges and provide intuition.</li>
<li>or describe a high-level topic and mention their half-baked ideas around the topic.</li>
</ul>
</p><p>The leader can choose however they want to present the background, it can be slides, notes, going over a paper or even just speaking. After presenting the background, there will be a ~40 minutes discussion. The leader should lead the discussion and encourage the engagement of participants by:</p>
<ul>
<li>Highlighting possible points of discussion/goals to achieve during the session.</li>
<li>Asking questions, example: Of the ideas presented/discussed, is there something you want to know more about?</li>
<li>Ask participants to brainstorm ideas</li>
</ul>
<p>
The last ~5 minutes of every meeting is spent to select the topic of the next meeting based on the votes and interests of the participants (available in the excel or website). 
</p>
<p>In each session there will also be a facilitator. The role of the facilitator is as follows:</p>
<ul>
<li>Introduce the leader and their area of expertise.</li>
<li>Encourage inclusive (rather than unilateral) discussions.</li>
<li>Ask a participant for their opinion on a specific question.</li>
<li>Perhaps call a participant’s name in chat and ask them to chime in and explain their ideas.</li>
<li>Ensure that everyone who wants to speak up gets a chance to do so, and no voices are left out.</li>
</ul>
</p>

<p>
<b>Note</b>: The goal is to have meetings that are stand-alone so that if someone missed one meeting, they would not have to worry about not being able to follow the next meeting. 
</p>


<h3>Schedule </h3>

      
<br>
<table id="schedule_table" style="width:100%;border: 2px solid #bbb; text-align:left;">
<tbody><tr>
</tr>
<tr>
<td style="text-align:left;background: #323E4C;font-size:17px;color:#fff;">Date </td>
<td style="text-align:left;background: #323E4C;font-size:17px;color:#fff;">Leader</td>
<td style="text-align:left;background: #323E4C;font-size:17px;color:#fff;">Topic</td>
<td style="text-align:left;background: #323E4C;font-size:17px;color:#fff;">Resources</td>
</tr>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Jan 25 2017 </td>
<td> Jason Jo </td>
<td> Understanding deep learning requires rethinking generalization </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;">Feb 8 2017 </td>
<td> Jason Jo </td>
<td> Train faster, generalize better: Stability of stochastic gradient descent </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Mar 8 2017 </td>
<td> Jason Jo </td>
<td> Entropy-SGD </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Apr 19 2017</td>
<td> Joseph Cohen </td>
<td> Early Stopping Without a Validation Set </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Nov 15 2017 </td>
<td> Brady Neal </td>
<td> Generalization in Deep Learning </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Nov 22 2017 </td>
<td> Anirudh Goyal </td>
<td> Information Bottleneck </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Nov 29 2017 </td>
<td> Sherjil Ozair </td>
<td> Generalization in GANs Slides </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Dec 13 2017 </td>
<td> Aristide Baratin </td>
<td> PAC-Bayes Generalization Slides </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>


<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Jan 15 2018 </td>
<td> Mike Pieper</td>
<td>Landscape of the Empirical Risk in Deep Learning </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Jan 22 2018 </td>
  <td> Ahmed Touati</td>
<td>A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Jan 29 2018 </td>
  <td> Ahmed Touati</td>
<td>Exploring Generalization in Deep Learning </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC"><td style="text-decoration: line-through;"> Feb 5 2018 </td>
<td> Jean Michel</td>
<td>Sellier Why does deep and cheap learning work so well? </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Feb 12 2018 </td>
<td> Brady Neal </td>
<td> Deep Learning without Poor Local Minima </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Feb 19 2018 </td>
<td> Rémi Le Priol</td>
<td>Concentration Inequalities Tutorial </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Feb 26 2018 </td>
  <td> Salem Lahlou</td>
<td>PAC-Bayes Tutorial </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Mar 12 2018 </td>
<td> Gabriel Huang</td>
<td>No Free Lunch Theorem Tutorial </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Mar 19 2018 </td>
 <td> Vidhi Jain</td>
<td>SGD Learns Networks that Provably Generalize on Linearly Separable Data </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;">Mar 26  2018 </td>
 <td> Matthew Scicluna
 </td>
<td>Revisit Understanding deep learning requires rethinking generalization  </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC"><td style="text-decoration: line-through;"> Apr 2 2018 </td>
<td> Ishmael Belghazi</td>
<td> MINE: Mutual Information Neural Estimation </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Apr 23 2018 </td>
 <td> Vincent Gripon </td>
<td>Matching Convolutional Neural Networks without Priors about Data </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC"><td style="text-decoration: line-through;"> May 7 2018 </td>
<td> Nicolas Gagné </td>
<td>Opening the Black Box of Deep Neural Networks via Information </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> May 14 2018 </td>
<td> Gaetan Marceau Caron</td>
<td> Do Deep Learning Models Have Too Many Parameters? </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>



<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Aug 13 2018 </td>
<td> Ari Benjamin</td>
<td> Measuring and regularizing networks in function space </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Aug 20 2018 </td>
<td> Jennifer She</td>
<td>Implicit Acceleration by Overparameterization </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Aug 27 2018 </td>
<td> Mohammad Pezeshki</td>
<td> Dynamics of Learning and Inference in Neural Networks </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>


<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Sep 10 2018 </td>
<td> Brady Neal </td>
<td> What is deep learning theory and why do we care? </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Sep 17 2018 </td>
<td> Rémi Le Priol </td>
<td> Empirical Analysis of the Hessian of Over-Parametrized Neural Networks </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;">Sept 24 2018 </td>
<td> Vikram Voleti </td>
<td> Visualizing the Loss Landscape of Neural Nets </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Oct 15 2018 </td>
<td> Brady Neal </td>
<td> Measuring the Intrinsic Dimension of Objective Landscapes </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Oct 22 2018 </td>
<td> Xavier Bouthillier </td>
<td> Understanding the Role of Over-Parametrization in Generalization </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Oct 29 2018 </td>
<td> César Laurent </td>
<td> Natural Gradient Tutorial </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC"><td style="text-decoration: line-through;"> Nov 5 2018 </td>
<td> Isabela Albuquerque </td>
<td> Data-Dependent Stability of Stochastic Gradient Descent </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Nov 12 2018 </td>
<td> Rémi Le Priol </td>
<td> The Mechanics of n-Player Differentiable Games </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Nov 19 2018 </td>
<td> Reyhane Askari</td>
<td>  A Lyapunov Analysis of Momentum Methods in Optimization </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;"> Nov 26 2018 </td>
<td> Levent Sagun </td>
<td>Over-paramertrization in neural networks: observations and a definition </td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;">an 29 2019 </td>
<td>Pablo Piantanida</td>
<td> Introduction to Information Theory - Part 1</td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;">Feb 5 2019 </td>
<td>Pablo Piantanida </td>
<td>Introduction to Information Theory - Part 2</td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;">Feb 12 2019 </td>
<td>Brady Neal </td>
<td> Discussion on bias-variance trade-off</td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;">Feb 19 2019</td>
<td> Sharan Vaswani </td>
<td>Train faster, generalize better: Stability of stochastic gradient descent</td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>

<tr style="background-color:#E6E9EC">
<td style="text-decoration: line-through;">Feb 26 2019 </td>
<td>Gauthier Gidel </td>
<td>Implicit Regularization of Gradient Dynamics in Linear Neural Networks</td>
<td><a href="https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory"> Link </a></td>


</tr>
</tbody></table>
<br>
<br>


<h3>Topics </h3>
<p>
Please enter your name and the topics of your interest in the spreadsheet provided in the slack channel. The topics of upcomming meetings will be selected from the list below. The list is in sync with the spreadsheet.
</p>

<br>
<table id="topics_table" style="width:100%;border: 2px solid #bbb; text-align:left;">
<tbody><tr>
</tr>
<tr>
<td style="text-align:left;background: #323E4C;font-size:17px;color:#fff;">Suggested by</td>
<td style="text-align:left;background: #323E4C;font-size:17px;color:#fff;">Topic</td>
<td style="text-align:left;background: #323E4C;font-size:17px;color:#fff;">Resources</td>
</tr>

<!-- <div> id="Name2"</div> -->

</tr>
</tbody></table>
<br>


<h3>Feedback & Contact</h3>
<p>
Please join <a href='https://join.slack.com/share/zt-fpcg0841-7i2sZPEdf~rzR8PgsSEwrA'> #deep-learning-theory</a> on Mila's slack for any suggestion and/or discussing different matters. Previous website and materials are accessible at <a href='https://sites.google.com/a/mila.quebec/reading-groups/groups/deep-theory'> Mila's internal site</a>. This website and the group are currently organized by <a href="https://scholar.google.ca/citations?user=rZvmqJsAAAAJ&hl=en"> Adam Ibrahim</a>, <a href="https://reyhaneaskari.github.io/"> Reyhane Askari</a>, and <a href="https://scholar.google.com/citations?user=HT85tXsAAAAJ&hl=en"> Mohammad Pezeshki</a>.
</p>


</div>
<div class="footer">
</div>

<script src="./files/tables.js"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/tabletop.js/1.5.1/tabletop.min.js'></script>

</body></html>
